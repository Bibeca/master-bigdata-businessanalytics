{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical case - Food Orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook only contains the function required by that statement with all models. For more information, see [02-food-orders-details](./02-food-orders-details.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statement (written in spanish) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejercicio se va a imaginar que se trabaja para una empresa de envíos de comida, presente\n",
    "en todo el territorio nacional, con miles de pedidos cada día. Dicha empresa tiene un fichero histórico\n",
    "con todas las peticiones de comida que los clientes han realizado mediante el chat de su web en los\n",
    "últimos meses. Necesitan analizar en tiempo real qué comidas están pidiendo los usuarios y qué\n",
    "ingredientes tenían, ya que en la cadena de stock de alimentos es necesario realizar una previsión para\n",
    "no quedarse sin platos cocinados. \n",
    "\n",
    "Se ha calculado que el impacto en las ventas cada vez que uno de\n",
    "los platos deja de estar disponible es del 7% de pérdidas en esa semana, debido al abandono de la web\n",
    "de pedidos por parte del cliente. Por tanto, es de vital importancia poder realizar automáticamente\n",
    "estimaciones al respecto.\n",
    "\n",
    "El objetivo es programar una función que reciba como input un texto de usuario y devuelva los\n",
    "fragmentos de texto (chunks) que hagan referencia a las comidas y cantidades que ha solicitado. No es\n",
    "necesario, ni es el objetivo de este ejercicio, construir un clasificador de intención previo a esta\n",
    "función, sino simplemente una función que presuponemos recibe una frase con la intención\n",
    "`Pedir_comida`. Tampoco es objetivo normalizar la salida (por ej.: no es necesario convertir 'tres' a '3'\n",
    "ni 'pizzas' a 'pizza'). Es, por tanto, un ejercicio de mínimos.\n",
    "\n",
    "    Por ejemplo: “quiero 3 bocadillos de anchoas y 2 pizzas” →\n",
    "    [\n",
    "        {comida:'bocadillo', ingrediente:'anchoas', cantidad:3},\n",
    "        {comida:'pizza', ingrediente:'null', cantidad:2}\n",
    "    ]\n",
    "    \n",
    "Por tanto, la salida de la función será un array con diccionarios de 2 elementos (`comida` y `cantidad`).\n",
    "Cuando una cantidad no sea detectada, se pondrá su valor a '1' como valor por defecto.\n",
    "\n",
    "Se deberá comenzar la práctica por el nivel más básico de dificultad (`RegexParser`) y, en caso de\n",
    "conseguirlo, añadir los siguientes niveles de forma sucesiva. De esta forma, el entregable contendrá\n",
    "todas y cada una de las tres formas de solucionar el problema. No basta, por tanto, con incluir, por\n",
    "ejemplo, únicamente un `NaiveBayesClassifier`, hay que incluir también las otras dos formas si se\n",
    "quiere obtener la máxima puntuación. Se trata simplemente de una práctica y, por tanto, no se espera\n",
    "como resultado un sistema de alta precisión listo para usar en producción, sino simplemente una\n",
    "aproximación básica que permita ejecutar las tres formas de resolver el problema.\n",
    "\n",
    "Este ejercicio hay que hacerlo con textos de entrenamiento en español, pero teniendo en cuenta que\n",
    "la precisión de los POS taggers en castellano de NLTK es muy mala. Por tanto, el alumno no debe\n",
    "frustrarse por no obtener buenos resultados, como hemos dicho anteriormente se trata simplemente de\n",
    "un ejercicio teórico y podemos suponer que, con un mejor analizador, podríamos obtener mejores\n",
    "resultados.\n",
    "\n",
    "Para llevar a cabo la práctica, deberá construirse una cadena NLP con NLTK, con los siguientes\n",
    "elementos:\n",
    "    - segmentación de frases,\n",
    "    - tokenización,\n",
    "    - POS tagger (analizador mofológico para el español).\n",
    "\n",
    "A continuación, los POS tags obtenidos serán usados por el `RegexParser`, el `UnigramParser`, el\n",
    "`BigramParser` y el `NaiveBayesClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import cess_esp\n",
    "\n",
    "\n",
    "# Define the getFoodOrders() function required by the statement. \n",
    "# That practical case ask us what food is deliveried and its amount. \n",
    "# That function is able to give it from a chunked sentence. \n",
    "# It is important to know that with that function, we assume the following structures in a sentence:\n",
    "\n",
    "    # 1. [...] Cantidad Comida [...]\n",
    "    # 2. [...] Comida [...]\n",
    "\n",
    "# In 2 case, we assume that `Cantidad = 1`. \n",
    "def getFoodOrders(chunked_sentence) :\n",
    "    delivery = []\n",
    "    dic = {}\n",
    "    default_cantidad = 1\n",
    "\n",
    "    chunks = [ chunk for chunk in nltk.chunk.tree2conlltags(chunked_sentence) \n",
    "                  if 'Cantidad' in chunk[2] or 'Comida' in chunk[2] ]\n",
    "    \n",
    "    i = 0\n",
    "    while (i < len(chunks)):\n",
    "        chunk = chunks[i]\n",
    "        w, t, c = chunk\n",
    "        \n",
    "        if c == 'B-Cantidad' :\n",
    "            dic['cantidad'] = w\n",
    "\n",
    "        if c == 'B-Comida' :\n",
    "            if 'cantidad' not in dic :\n",
    "                dic['cantidad'] = default_cantidad\n",
    "\n",
    "            dic['comida'] = w\n",
    "            \n",
    "            j = i+1\n",
    "                  \n",
    "            condition = (j < len(chunks))\n",
    "            while (condition) :\n",
    "                w, t, c = chunks[j]\n",
    "                \n",
    "                if c == 'I-Comida' :\n",
    "                    if 'ingredientes' not in dic :\n",
    "                        dic['ingredientes'] = w\n",
    "                    else :\n",
    "                        dic['ingredientes'] = dic['ingredientes'] + \" \" + w\n",
    "        \n",
    "                j += 1\n",
    "                condition = (j < len(chunks) and c == 'I-Comida')\n",
    "   \n",
    "            delivery.append(dic)\n",
    "            dic = {}\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "    return delivery\n",
    "\n",
    "\n",
    "\n",
    "# Define createIOBCorpus() function which give us the IOB Corpus from corpus\n",
    "# using 'pos_tagger' like a POS tagger and 'regex_parser' like a RegexParser \n",
    "def createIOBCorpus(corpus, pos_tagger, regex_parser) :\n",
    "\n",
    "    iob_corpus = []\n",
    "\n",
    "    # For each sentence in corpus\n",
    "    for sent in corpus :\n",
    "        # Tokenize the sentence\n",
    "        tokens = nltk.word_tokenize(sent)\n",
    "\n",
    "        # Tag the sentence\n",
    "        tagged_sent = pos_tagger.tag(tokens)\n",
    "\n",
    "        # Parse tagged_sent\n",
    "        chunked_sent = regex_parser.parse(tagged_sent)\n",
    "\n",
    "        iob_corpus.append(chunked_sent)\n",
    "    \n",
    "    return iob_corpus\n",
    "\n",
    "\n",
    "\n",
    "# Define UnigramChunker class\n",
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents): \n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.UnigramTagger(train_data) \n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "    \n",
    "    \n",
    "# Define BigramChunker class\n",
    "class BigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents): \n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data) \n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "    \n",
    "    \n",
    "# Define NaiveBayesChunker class\n",
    "class ConsecutivePosTagger(nltk.TaggerI):\n",
    "\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = pos_features(untagged_sent, i, history) \n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "                \n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = pos_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "\n",
    "class NaiveBayesChunker(nltk.ChunkParserI): \n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = [[((w,t),c) for (w,t,c) in\n",
    "                         nltk.chunk.tree2conlltags(sent)]\n",
    "                        for sent in train_sents]\n",
    "        self.tagger = ConsecutivePosTagger(tagged_sents)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "    \n",
    "    \n",
    "# Define pos_features() function using only the current POS tag of the word.\n",
    "def pos_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    return {\"pos\": pos}\n",
    "\n",
    "\n",
    "\n",
    "# Create initialize() function to initialize all models\n",
    "def initialize() :\n",
    "    \n",
    "    # Load all tagged sentences of Spanish CESS corpus\n",
    "    sents = cess_esp.tagged_sents()\n",
    "\n",
    "    # POS Tagger Training\n",
    "    # Split the Spanish CESS corpus between training and testing dataset\n",
    "    training = []\n",
    "    testing = []\n",
    "\n",
    "    for i in range(len(sents)) :\n",
    "        if i % 10 :\n",
    "            training.append(sents[i])\n",
    "        else :\n",
    "            testing.append(sents[i])\n",
    "\n",
    "    # Import HiddenMarkovModelTagger\n",
    "    from nltk.tag.hmm import HiddenMarkovModelTagger\n",
    "\n",
    "    # Create the Spanish POS tagger using HMM Tagger\n",
    "    spanish_pos_tagger = HiddenMarkovModelTagger.train(training)\n",
    "    \n",
    "    # Define the grammar for RegexParser\n",
    "    grammar = r\"\"\"\n",
    "        Comida: {<s?n[cp\\.].*><s.*>?<s?[na][cp\\.].*>*}\n",
    "                {<s?n[cp\\.].*>}\n",
    "        Cantidad: {<d[in].*>}\n",
    "                  {<Z.*>}  \n",
    "    \"\"\"\n",
    "\n",
    "    # Create the RegexParser\n",
    "    regex_parser = nltk.RegexpParser(grammar)\n",
    "    \n",
    "    \n",
    "    # Create a corpus with sentences for UnigramChunker, BigramChunker and NaiveBayesChunker training.\n",
    "    corpus = [\n",
    "        \"Me gustaría comer una tortilla de patatas\",\n",
    "        \"¿Nos pones 3 tocinos, 4 pechugas?\",\n",
    "        \"5 repollos, 12 sardinas y 8 jalapeños\",\n",
    "        \"Estamos indecisos, pero puedes ponernos mientras tres raciones de patatas fritas\",\n",
    "        \"¿Sería tan amable de servirnos catorce tostadas?\",\n",
    "        \"Queremos 2 pollos con caracoles, una ternera, tres patos y 2 pimientos con arroz\",\n",
    "        \"Nuestro pedido es: 10 hamburguesas con queso, 20 pizzas y 3 patatas\",\n",
    "        \"Yo quiero 9 yogures, 12 ensaladas, un pimiento, 4 chorizos, 12 empanadillas de atún, 8 crespillos y 3 alubias\",\n",
    "        \"Me gustaría comer: kiwi con ensalada\",\n",
    "        \"Él quiere pedir 3 tostadas, 2 tomates y 6 uvas\",\n",
    "        \"Ellos han pedido 2 higos, 1 salmón con caracoles y arroz\",\n",
    "        \"Tú has pedido 4 fajitas de pollo, dos emperadores con patatas y tres rúculas\",\n",
    "        \"2 macarrones, 3 quesos, una guindilla, cuatro pulpos y quince verduras\",\n",
    "        \"Mi abuela quiere emperador con ensalada\",\n",
    "        \"Mi tía quiere comer marisco y carne\",\n",
    "        \"2 cebollas, una calabaza y ocho tomates\",\n",
    "        \"tres pollos, dos corderos, cuatro cerdos y tres macarrones\",\n",
    "        \"Pepinillos, calabaza, navajas, sepia y gulas\",\n",
    "        \"Me gustaría comer bonito a la plancha con patatas asadas\",\n",
    "        \"Pídete cinco pomelos, tres calabacines, 8 mangos, seis melocotones\"\n",
    "        \"Langosta, langostinos y almejas\"\n",
    "    ]\n",
    "\n",
    "    # Create IOB Corpus\n",
    "    iob_corpus = createIOBCorpus(corpus, spanish_pos_tagger, regex_parser)\n",
    "\n",
    "    \n",
    "    # Create UnigramChunker\n",
    "    unigram_chunker = UnigramChunker(iob_corpus)\n",
    "    \n",
    "    # Create BigramChunker\n",
    "    bigram_chunker = BigramChunker(iob_corpus)\n",
    "    \n",
    "    # Create NaiveBayesChunker\n",
    "    naive_chunker = NaiveBayesChunker(iob_corpus)\n",
    "    \n",
    "    return spanish_pos_tagger, regex_parser, unigram_chunker, bigram_chunker, naive_chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type the food order (in spanish):\n",
      "(type 'quit' for exit)\n",
      "Yo quiero tres arroces con pollo, cuatro sopas y 14 pepinos\n",
      "The food order is:\n",
      "RegexParser:\n",
      "[{'cantidad': 'tres', 'comida': 'arroces', 'ingredientes': 'con pollo'}, {'cantidad': 'cuatro', 'comida': 'sopas'}, {'cantidad': '14', 'comida': 'pepinos'}]\n",
      "\n",
      "UnigramChunker:\n",
      "[{'cantidad': 'tres', 'comida': 'arroces', 'ingredientes': 'con'}, {'cantidad': 1, 'comida': 'pollo'}, {'cantidad': 'cuatro', 'comida': 'sopas'}, {'cantidad': '14', 'comida': 'pepinos'}]\n",
      "\n",
      "BigramChunker:\n",
      "[{'cantidad': 'tres', 'comida': 'arroces', 'ingredientes': 'con pollo'}, {'cantidad': 'cuatro', 'comida': 'sopas'}, {'cantidad': '14', 'comida': 'pepinos'}]\n",
      "\n",
      "NaiveBayesChunker:\n",
      "[{'cantidad': 'tres', 'comida': 'arroces', 'ingredientes': 'con'}, {'cantidad': 1, 'comida': 'pollo'}, {'cantidad': 'cuatro', 'comida': 'sopas'}, {'cantidad': '14', 'comida': 'pepinos'}]\n",
      "\n",
      "-------------------\n",
      "\n",
      "Type the food order (in spanish):\n",
      "(type 'quit' for exit)\n",
      "Ellos van a pedir un guiso de carne y dos ensaladas\n",
      "The food order is:\n",
      "RegexParser:\n",
      "[{'cantidad': 'un', 'comida': 'guiso', 'ingredientes': 'de carne'}, {'cantidad': 'dos', 'comida': 'ensaladas'}]\n",
      "\n",
      "UnigramChunker:\n",
      "[{'cantidad': 1, 'comida': 'a'}, {'cantidad': 'un', 'comida': 'guiso', 'ingredientes': 'de'}, {'cantidad': 1, 'comida': 'carne'}, {'cantidad': 'dos', 'comida': 'ensaladas'}]\n",
      "\n",
      "BigramChunker:\n",
      "[]\n",
      "\n",
      "NaiveBayesChunker:\n",
      "[{'cantidad': 1, 'comida': 'a'}, {'cantidad': 'un', 'comida': 'guiso', 'ingredientes': 'de'}, {'cantidad': 1, 'comida': 'carne'}, {'cantidad': 'dos', 'comida': 'ensaladas'}]\n",
      "\n",
      "-------------------\n",
      "\n",
      "Type the food order (in spanish):\n",
      "(type 'quit' for exit)\n",
      "quit\n",
      "\n",
      "Thank you, good bye!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\" :\n",
    "    pos_tagger, regex_parser, unigram_chunker, bigram_chunker, naive_chunker = initialize()\n",
    "    \n",
    "    condition = True\n",
    "    \n",
    "    while (condition) :\n",
    "        sentence = input(\"Type the food order (in spanish):\\n(type 'quit' for exit)\\n\")\n",
    "        \n",
    "        condition = (sentence != \"quit\")\n",
    "        \n",
    "        if condition :\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            tagged = pos_tagger.tag(tokens)\n",
    "\n",
    "            parsed = naive_chunker.parse(tagged)\n",
    "\n",
    "            regex = regex_parser.parse(tagged)\n",
    "            unigram = unigram_chunker.parse(tagged)\n",
    "            bigram = bigram_chunker.parse(tagged)\n",
    "            naive = naive_chunker.parse(tagged)\n",
    "\n",
    "            print(\"The food order is:\")\n",
    "            print(\"RegexParser:\")\n",
    "            print(getFoodOrders(regex))\n",
    "\n",
    "            print()\n",
    "            print(\"UnigramChunker:\")\n",
    "            print(getFoodOrders(unigram))\n",
    "\n",
    "            print()\n",
    "            print(\"BigramChunker:\")\n",
    "            print(getFoodOrders(bigram))\n",
    "\n",
    "            print()\n",
    "            print(\"NaiveBayesChunker:\")\n",
    "            print(getFoodOrders(naive))\n",
    "            \n",
    "            print()\n",
    "            print(\"-------------------\")\n",
    "            print()\n",
    "        else :\n",
    "            print()\n",
    "            print(\"Thank you, good bye!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
