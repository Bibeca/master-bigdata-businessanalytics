{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining - Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply clustering using `K-Means` with `tfidf` vectorizer, we are going to use the example into that [URL](http://scikit-learn.org/stable/auto_examples/text/document_clustering.html). That example use a vectorizer to getting the `tfidf` of all words in a document. This vectorizer is `TfidfVectorizer`.\n",
    "\n",
    "We are going to use the [20newgroups](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html#newsgroups) corpus and select two group: `alt.atheis` and `sci.space`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import nltk\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the corpus of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'sci.space']\n",
      "\n",
      "1786 documents\n"
     ]
    }
   ],
   "source": [
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'sci.space'\n",
    "]\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "print()\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                             shuffle=True, random_state=42)\n",
    "\n",
    "trainCorpus = dataset.data\n",
    "print(\"%d documents\" % len(trainCorpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Texts vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tf Matrix:\n",
      " [[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "number of sentences 1786, number of features 28382\n"
     ]
    }
   ],
   "source": [
    "# Create the vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Use the vectorizer to transform the documents on a matrix of tf's (term frequency) of documents\n",
    "vectorizer.fit(trainCorpus)\n",
    "\n",
    "# Extract the terms frequency\n",
    "tfMatrix = vectorizer.transform(trainCorpus)\n",
    "\n",
    "# Print the matrix. \n",
    "# This matrix converted in array indicates: \n",
    "#    - Each column is a one feature, \n",
    "#    - Each row is a one sentence of corpus.\n",
    "#    - The (i,j) value indicates the frequency of j feature in a i sentence\n",
    "print('\\ntf Matrix:\\n', tfMatrix.toarray())\n",
    "\n",
    "# Print the shape of our matrix:\n",
    "print(\"number of sentences %d, number of features %d\" % tfMatrix.shape)\n",
    "\n",
    "# Get all features \n",
    "tf_feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Apply LDA Algorthim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 28382)\n",
      "(1786, 10)\n"
     ]
    }
   ],
   "source": [
    "topics = 10\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components =topics, max_iter=5, \n",
    "                                      learning_method='online', \n",
    "                                      learning_offset=50.,\n",
    "                                      random_state=0).fit(tfMatrix)\n",
    "\n",
    "# Variational parameters for topic word distribution. \n",
    "#  Since the complete conditional for topic word distribution is a Dirichlet, \n",
    "#  components_[i, j] can be viewed as pseudocount that represents the number \n",
    "#  of times word j was assigned to topic i. It can also be viewed as distribution \n",
    "#  over the words for each topic after normalization:\n",
    "topic_word_distribution = lda_model.components_\n",
    "\n",
    "# Document topic distribution for tfMatrix.\n",
    "document_topic_distribution = lda_model.transform(tfMatrix)\n",
    "\n",
    "print(topic_word_distribution.shape)\n",
    "print(document_topic_distribution.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display topics depart from:\n",
    "#  - H: Variational parameters for topic word distribution.\n",
    "#  - W: Document topic distribution.\n",
    "#  - feature_names: Names of each corpus feature\n",
    "#  - documents: Corpus\n",
    "#  - no_top_words: Maximum number of words (topic) to display\n",
    "#  - no_top_documents: Maximum number of corpus document to display\n",
    "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print(\"\\nTopic %d:\" % (topic_idx))\n",
    "        for i in topic.argsort()[:-no_top_words - 1:-1]:\n",
    "            print(\" \",feature_names[i])\n",
    "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        for doc_index in top_doc_indices:\n",
    "            print(documents[doc_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0:\n",
      "  af\n",
      "  afit\n",
      "  mil\n",
      "  elements\n",
      "\n",
      "Topic 1:\n",
      "  het\n",
      "  een\n",
      "  van\n",
      "  te\n",
      "\n",
      "Topic 2:\n",
      "  kadie\n",
      "  of\n",
      "  the\n",
      "  pto\n",
      "\n",
      "Topic 3:\n",
      "  space\n",
      "  launch\n",
      "  satellite\n",
      "  pub\n",
      "\n",
      "Topic 4:\n",
      "  arms\n",
      "  the\n",
      "  permanet\n",
      "  prado\n",
      "\n",
      "Topic 5:\n",
      "  de\n",
      "  van\n",
      "  het\n",
      "  een\n",
      "\n",
      "Topic 6:\n",
      "  the\n",
      "  of\n",
      "  to\n",
      "  and\n",
      "\n",
      "Topic 7:\n",
      "  the\n",
      "  of\n",
      "  alcbel\n",
      "  devdjn\n",
      "\n",
      "Topic 8:\n",
      "  wam\n",
      "  the\n",
      "  comet\n",
      "  emr\n",
      "\n",
      "Topic 9:\n",
      "  the\n",
      "  globe\n",
      "  xpresso\n",
      "  of\n"
     ]
    }
   ],
   "source": [
    "no_top_words = 4\n",
    "no_top_documents = 0\n",
    "\n",
    "display_topics(topic_word_distribution, document_topic_distribution, \n",
    "               tf_feature_names, trainCorpus, no_top_words, no_top_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Apply NMF Algorthim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 28382)\n",
      "(1786, 10)\n"
     ]
    }
   ],
   "source": [
    "topics = 10\n",
    "\n",
    "lda_model = NMF(n_components =topics, max_iter=5, \n",
    "                random_state=0).fit(tfMatrix)\n",
    "\n",
    "# Variational parameters for topic word distribution. \n",
    "#  Since the complete conditional for topic word distribution is a Dirichlet, \n",
    "#  components_[i, j] can be viewed as pseudocount that represents the number \n",
    "#  of times word j was assigned to topic i. It can also be viewed as distribution \n",
    "#  over the words for each topic after normalization:\n",
    "topic_word_distribution = lda_model.components_\n",
    "\n",
    "# Document topic distribution for tfMatrix.\n",
    "document_topic_distribution = lda_model.transform(tfMatrix)\n",
    "\n",
    "print(topic_word_distribution.shape)\n",
    "print(document_topic_distribution.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display topics depart from:\n",
    "#  - H: Variational parameters for topic word distribution.\n",
    "#  - W: Document topic distribution.\n",
    "#  - feature_names: Names of each corpus feature\n",
    "#  - documents: Corpus\n",
    "#  - no_top_words: Maximum number of words (topic) to display\n",
    "#  - no_top_documents: Maximum number of corpus document to display\n",
    "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print(\"\\nTopic %d:\" % (topic_idx))\n",
    "        for i in topic.argsort()[:-no_top_words - 1:-1]:\n",
    "            print(\" \",feature_names[i])\n",
    "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        for doc_index in top_doc_indices:\n",
    "            print(documents[doc_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0:\n",
      "  the\n",
      "  of\n",
      "  and\n",
      "  to\n",
      "\n",
      "Topic 1:\n",
      "  that\n",
      "  is\n",
      "  it\n",
      "  to\n",
      "\n",
      "Topic 2:\n",
      "  and\n",
      "  space\n",
      "  for\n",
      "  launch\n",
      "\n",
      "Topic 3:\n",
      "  is\n",
      "  of\n",
      "  the\n",
      "  argument\n",
      "\n",
      "Topic 4:\n",
      "  you\n",
      "  to\n",
      "  your\n",
      "  and\n",
      "\n",
      "Topic 5:\n",
      "  the\n",
      "  in\n",
      "  venus\n",
      "  on\n",
      "\n",
      "Topic 6:\n",
      "  to\n",
      "  atheists\n",
      "  atheism\n",
      "  god\n",
      "\n",
      "Topic 7:\n",
      "  of\n",
      "  in\n",
      "  larson\n",
      "  universe\n",
      "\n",
      "Topic 8:\n",
      "  nasa\n",
      "  on\n",
      "  space\n",
      "  to\n",
      "\n",
      "Topic 9:\n",
      "  edu\n",
      "  in\n",
      "  writes\n",
      "  it\n"
     ]
    }
   ],
   "source": [
    "no_top_words = 4\n",
    "no_top_documents = 0\n",
    "\n",
    "display_topics(topic_word_distribution, document_topic_distribution, \n",
    "               tf_feature_names, trainCorpus, no_top_words, no_top_documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
