{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks - Optical Character Recognition (OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Theano "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MKL_THREADING_LAYER=GNU\n"
     ]
    }
   ],
   "source": [
    "env MKL_THREADING_LAYER=GNU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MNIST database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "datasets_dir = 'data/'\n",
    "\n",
    "def one_hot(x,n):\n",
    "    if type(x) == list:\n",
    "        x = np.array(x)\n",
    "    x = x.flatten()\n",
    "    o_h = np.zeros((len(x),n))\n",
    "    o_h[np.arange(len(x)),x] = 1\n",
    "    return o_h\n",
    "\n",
    "def mnist(ntrain=60000,ntest=10000,onehot=True):\n",
    "    data_dir = os.path.join(datasets_dir,'mnist/')\n",
    "    fd = open(os.path.join(data_dir,'train-images-idx3-ubyte'))\n",
    "    loaded = np.fromfile(file=fd,dtype=np.uint8)\n",
    "    trX = loaded[16:].reshape((60000,28*28)).astype(float)\n",
    "\n",
    "    fd = open(os.path.join(data_dir,'train-labels-idx1-ubyte'))\n",
    "    loaded = np.fromfile(file=fd,dtype=np.uint8)\n",
    "    trY = loaded[8:].reshape((60000))\n",
    "\n",
    "    fd = open(os.path.join(data_dir,'t10k-images-idx3-ubyte'))\n",
    "    loaded = np.fromfile(file=fd,dtype=np.uint8)\n",
    "    teX = loaded[16:].reshape((10000,28*28)).astype(float)\n",
    "\n",
    "    fd = open(os.path.join(data_dir,'t10k-labels-idx1-ubyte'))\n",
    "    loaded = np.fromfile(file=fd,dtype=np.uint8)\n",
    "    teY = loaded[8:].reshape((10000))\n",
    "\n",
    "    trX = trX/255.\n",
    "    teX = teX/255.\n",
    "\n",
    "    trX = trX[:ntrain]\n",
    "    trY = trY[:ntrain]\n",
    "\n",
    "    teX = teX[:ntest]\n",
    "    teY = teY[:ntest]\n",
    "\n",
    "    if onehot:\n",
    "        trY = one_hot(trY, 10)\n",
    "        teY = one_hot(teY, 10)\n",
    "    else:\n",
    "        trY = np.asarray(trY)\n",
    "        teY = np.asarray(teY)\n",
    "\n",
    "    return trX,teX,trY,teY\n",
    "\n",
    "trX, teX, trY, teY = mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1052c19668>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(trY[1,:])\n",
    "imshow(trX[1,:].reshape(28, 28), cmap = mpl.cm.gray)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem's analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before implementing a neural network which can classify the images of dataset, it is necessary to analyse the shape of the input and output layers. The input layer is definied by the image's size, in this case, each image has 784 pixels, so the input layer's size will be 784. On the other hand, the output layer is definied by the characters to identify, in this case, the characters to identify are 10 (each one is one number from 0 to 9), so the output layer's size will be 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### First solution. One layer: 10 neurons, with 784 inputs each one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define functions for init the weights the random way:\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype = theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    return theano.shared(floatX(np.random.randn(*shape) * 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "trX, teX, trY, teY = mnist()\n",
    "\n",
    "# Create the symbolic variables\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize the weights\n",
    "w = init_weights((784, 10)) # 10 neurons with 784 inputs\n",
    "\n",
    "# Create the neurons\n",
    "py_x = T.nnet.sigmoid(T.dot(X, w)) # Let's use the sigmoid neurons\n",
    "\n",
    "# Create the output of neuralnet -> for prediction\n",
    "# The output of neuralnet is the maximum value of the all neurons\n",
    "y_pred = T.argmax(py_x, axis = 1)\n",
    "\n",
    "# Create the effort function\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(py_x, Y))\n",
    "gradient = T.grad(cost, w)\n",
    "\n",
    "# Create the update function\n",
    "update = [[w, w - gradient * learning_rate]]\n",
    "\n",
    "# Create the traning function\n",
    "train = theano.function(inputs = [X, Y],\n",
    "                        outputs = cost,\n",
    "                        updates = update,\n",
    "                        allow_input_downcast = True)    # Make the data types conversion of \n",
    "                                                        # input data so that it will be adapted \n",
    "                                                        # to data types needed in the model\n",
    "\n",
    "# Create the prediction function\n",
    "predict = theano.function(inputs = [X],\n",
    "                          outputs = y_pred,\n",
    "                          allow_input_downcast = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.721 0.0027029250705039904\n",
      "1 0.7027 0.0012359935650084043\n",
      "2 0.6929 0.0007827126020154541\n",
      "3 0.6842 0.0005661670688884367\n",
      "4 0.6782 0.0004404375966753246\n",
      "5 0.6749 0.0003587461438120202\n",
      "6 0.6713 0.00030161591541922066\n",
      "7 0.6677 0.0002595307049547664\n",
      "8 0.6649 0.00022730458286666656\n",
      "9 0.6627 0.00020187703911309947\n",
      "10 0.6611 0.00018132851666172823\n",
      "11 0.6599 0.0001643957286224651\n",
      "12 0.6585 0.00015021421087009279\n",
      "13 0.6565 0.0001381729949705559\n",
      "14 0.6559 0.00012782837096527995\n",
      "15 0.6552 0.00011885053112700169\n",
      "16 0.6544 0.00011098936515787504\n",
      "17 0.654 0.00010405185281010549\n",
      "18 0.6525 9.788671947520187e-05\n",
      "19 0.652 9.237377559188429e-05\n",
      "20 0.6514 8.741635558006916e-05\n",
      "21 0.6504 8.293585544184047e-05\n",
      "22 0.6494 7.886772073380761e-05\n",
      "23 0.6485 7.515845551924303e-05\n",
      "24 0.648 7.176336209440906e-05\n"
     ]
    }
   ],
   "source": [
    "num_iter = 25\n",
    "\n",
    "# Training the model\n",
    "for i in range(num_iter) :\n",
    "    for start, end in zip(range(0, len(trX), 128), range(128, len(trX), 128)) :\n",
    "        cost = train(trX[start:end], trY[start:end])\n",
    "    \n",
    "    # Make the prediction\n",
    "    pred = predict(teX)\n",
    "    # Print the result\n",
    "    print(i, np.mean(np.argmax(teY, axis = 1) == pred), cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With 25 iterations, the model has got a precision of 64.33%.\n",
    "\n",
    "One way to improve this model is modify the neuron's type. In the last example, we used the `sigmoid` neuron. Now, we are going to use the `softmax` neuron. The implementation with this neuron is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "trX, teX, trY, teY = mnist()\n",
    "\n",
    "# Create the symbolic variables\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize the weights\n",
    "w = init_weights((784, 10)) # 10 neurons with 784 inputs\n",
    "\n",
    "# Create the neurons\n",
    "py_x = T.nnet.softmax(T.dot(X, w)) # Let's use the softmax neurons\n",
    "\n",
    "# Create the output of neuralnet -> for prediction\n",
    "# The output of neuralnet is the maximum value of the all neurons\n",
    "y_pred = T.argmax(py_x, axis = 1)\n",
    "\n",
    "# Create the effort function\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(py_x, Y))\n",
    "gradient = T.grad(cost, w)\n",
    "\n",
    "# Create the update function\n",
    "update = [[w, w - gradient * learning_rate]]\n",
    "\n",
    "# Create the traning function\n",
    "train = theano.function(inputs = [X, Y],\n",
    "                        outputs = cost,\n",
    "                        updates = update,\n",
    "                        allow_input_downcast = True)    # Make the data types conversion of \n",
    "                                                        # input data so that it will be adapted \n",
    "                                                        # to data types needed in the model\n",
    "\n",
    "# Create the prediction function\n",
    "predict = theano.function(inputs = [X],\n",
    "                          outputs = y_pred,\n",
    "                          allow_input_downcast = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8976 0.1410743366750079\n",
      "1 0.9072 0.11161650977249434\n",
      "2 0.911 0.09968903727711292\n",
      "3 0.9131 0.09282863142173241\n",
      "4 0.9152 0.08825177029847385\n",
      "5 0.916 0.08493602419833998\n",
      "6 0.9176 0.08240406134611025\n",
      "7 0.9185 0.08039836474664207\n",
      "8 0.9191 0.07876580948445258\n",
      "9 0.9196 0.07740875931823077\n",
      "10 0.9196 0.07626160793535955\n",
      "11 0.9198 0.07527843514866621\n",
      "12 0.9199 0.07442603004817548\n",
      "13 0.9202 0.07367971764752164\n",
      "14 0.9202 0.07302074549995513\n",
      "15 0.92 0.0724345843052235\n",
      "16 0.9208 0.07190978738408238\n",
      "17 0.9213 0.07143720432738752\n",
      "18 0.9216 0.07100942602801297\n",
      "19 0.9219 0.07062038488929062\n",
      "20 0.9219 0.0702650615140738\n",
      "21 0.9221 0.06993926595820882\n",
      "22 0.9221 0.06963947215796719\n",
      "23 0.922 0.0693626909077854\n",
      "24 0.922 0.06910637121026347\n"
     ]
    }
   ],
   "source": [
    "num_iter = 25\n",
    "\n",
    "# Training the model\n",
    "for i in range(num_iter) :\n",
    "    for start, end in zip(range(0, len(trX), 128), range(128, len(trX), 128)) :\n",
    "        cost = train(trX[start:end], trY[start:end])\n",
    "    \n",
    "    # Make the prediction\n",
    "    pred = predict(teX)\n",
    "    # Print the result\n",
    "    print(i, np.mean(np.argmax(teY, axis = 1) == pred), cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can observe that the results are better with the `softmax` neuron. Now, the model has got a precision of 92.19%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Second solution. Two layers: 625 neurons with 784 inputs each one, and 10 neurons with 625 inputs each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define functions for init the weights the random way:\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype = theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    return theano.shared(floatX(np.random.randn(*shape) * 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "trX, teX, trY, teY = mnist()\n",
    "\n",
    "# Create the symbolic variables\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize the weights\n",
    "w_h = init_weights((784, 625)) # 625 neurons with 784 inputs (hide layer) \n",
    "w_o = init_weights((625, 10))  # 10 neurons with 625 inputs  (output layer)\n",
    "\n",
    "# Create the neurons\n",
    "h = T.nnet.sigmoid(T.dot(X, w_h))    # For the hide layer, let's use the sigmoid\n",
    "py_x = T.nnet.softmax(T.dot(h, w_o)) # Let's use the softmax neurons with the result of the hide layer\n",
    "\n",
    "# Create the output of neuralnet -> for prediction\n",
    "# The output of neuralnet is the maximum value of the all neurons\n",
    "y_pred = T.argmax(py_x, axis = 1)\n",
    "\n",
    "# Create the effort function\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(py_x, Y))\n",
    "dw_h = T.grad(cost, w_h)\n",
    "dw_o = T.grad(cost, w_o)\n",
    "\n",
    "# Create the update function\n",
    "updates = [[w_h, w_h - dw_h * learning_rate],\n",
    "           [w_o, w_o - dw_o * learning_rate]]\n",
    "\n",
    "# Create the traning function\n",
    "train = theano.function(inputs = [X, Y],\n",
    "                        outputs = cost,\n",
    "                        updates = updates,\n",
    "                        allow_input_downcast = True)    # Make the data types conversion of \n",
    "                                                        # input data so that it will be adapted \n",
    "                                                        # to data types needed in the model\n",
    "\n",
    "# Create the prediction function\n",
    "predict = theano.function(inputs = [X],\n",
    "                          outputs = y_pred,\n",
    "                          allow_input_downcast = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.823 0.3734138127327109\n",
      "1 0.8771 0.1527132943075869\n",
      "2 0.8907 0.11456293864760102\n",
      "3 0.8977 0.09880993861258901\n",
      "4 0.9041 0.08944247557613755\n",
      "5 0.9066 0.08281563473800227\n",
      "6 0.9091 0.07771112933084755\n",
      "7 0.9116 0.07361295555864907\n",
      "8 0.9131 0.0702472972687059\n",
      "9 0.9142 0.06743981992373496\n",
      "10 0.915 0.06506632191559396\n",
      "11 0.9169 0.06303284264473215\n",
      "12 0.9179 0.06126611477910101\n",
      "13 0.9191 0.059708194405558516\n",
      "14 0.9205 0.058313027169474216\n",
      "15 0.9222 0.05704404029054002\n",
      "16 0.9226 0.05587235159832256\n",
      "17 0.9242 0.054775393138720846\n",
      "18 0.9249 0.05373583559280131\n",
      "19 0.9264 0.05274072782382834\n",
      "20 0.9284 0.05178076132947119\n",
      "21 0.9293 0.05084955845688987\n",
      "22 0.9306 0.04994289401470391\n",
      "23 0.9323 0.049057830308201954\n",
      "24 0.9337 0.04819189382381959\n"
     ]
    }
   ],
   "source": [
    "num_iter = 25\n",
    "\n",
    "# Training the model\n",
    "for i in range(num_iter) :\n",
    "    for start, end in zip(range(0, len(trX), 128), range(128, len(trX), 128)) :\n",
    "        cost = train(trX[start:end], trY[start:end])\n",
    "    \n",
    "    # Make the prediction\n",
    "    pred = predict(teX)\n",
    "    # Print the result\n",
    "    print(i, np.mean(np.argmax(teY, axis = 1) == pred), cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The precision got is the 93.4%. It is better than the last, although not much better because the precision already was high. As we increase the number of neurons and layers in a neuralnet is easy that the model will be unstable and overfitting can appear. This problem can be fixed using regularization techniques like `dropout`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Regularization techniques: Dropout "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This technique consists in turn off a percentage of neurons randomly during the training, like this, the contribution of these neurons in the model's training will not be considered and its weights will not be updates through backpropagation.\n",
    "\n",
    "The implementation of this technique is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano.sandbox.rng_mrg import MRG_RandomStreams\n",
    "\n",
    "srng = MRG_RandomStreams() # Random values generator\n",
    "\n",
    "# Dropout function's definition\n",
    "def dropout(X, p):\n",
    "    if p > 0:\n",
    "        X *= srng.binomial(X.shape, p = 1 - p, dtype = theano.config.floatX)\n",
    "        X /= 1 - p\n",
    "    return X\n",
    "\n",
    "# Model's creation.\n",
    "#  X: input\n",
    "#  w_h: hide layer's weights\n",
    "#  w_o: output layer's weights\n",
    "#  p_drop: probability of the neuron's turn off\n",
    "def model(X, w_h, w_o, p_drop):\n",
    "    X = dropout(X, p_drop)\n",
    "    h = T.nnet.sigmoid(T.dot(X, w_h))\n",
    "    \n",
    "    h = dropout(h, p_drop)\n",
    "    py_x = T.nnet.softmax(T.dot(h, w_o))\n",
    "\n",
    "    return h, py_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define functions for init the weights the random way:\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype = theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    return theano.shared(floatX(np.random.randn(*shape) * 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "trX, teX, trY, teY = mnist()\n",
    "\n",
    "# Create the symbolic variables\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize the weights\n",
    "w_h = init_weights((784, 625)) # 625 neurons with 784 inputs (hide layer) \n",
    "w_o = init_weights((625, 10))  # 10 neurons with 625 inputs  (output layer)\n",
    "\n",
    "# Create the training model with the 0.05 probability of the neurons turn off\n",
    "h, py_x = model(X, w_h, w_o, 0.05)\n",
    "\n",
    "# Create the effort function\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(py_x, Y))\n",
    "dw_h = T.grad(cost, w_h)\n",
    "dw_o = T.grad(cost, w_o)\n",
    "\n",
    "# Create the update function\n",
    "updates = [[w_h, w_h - dw_h * learning_rate],\n",
    "           [w_o, w_o - dw_o * learning_rate]]\n",
    "\n",
    "# Create the traning function\n",
    "train = theano.function(inputs = [X, Y],\n",
    "                        outputs = cost,\n",
    "                        updates = updates,\n",
    "                        allow_input_downcast = True)    # Make the data types conversion of \n",
    "                                                        # input data so that it will be adapted \n",
    "                                                        # to data types needed in the model\n",
    "\n",
    "# Create the prediction model with the 0 probability of the neurons turn off\n",
    "h_pred, py_pred = model(X, w_h, w_o, 0.0)\n",
    "y_pred = T.argmax(py_pred, axis = 1)\n",
    "\n",
    "\n",
    "# Create the prediction function\n",
    "predict = theano.function(inputs = [X],\n",
    "                          outputs = y_pred,\n",
    "                          allow_input_downcast = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8187 0.3899279482807628\n",
      "1 0.875 0.1710710303370585\n",
      "2 0.8911 0.11364033990474715\n",
      "3 0.8999 0.10564864778826542\n",
      "4 0.9038 0.08954041223557303\n",
      "5 0.9068 0.10605462035689366\n",
      "6 0.9111 0.07906449620753736\n",
      "7 0.9115 0.08267554004644682\n",
      "8 0.9134 0.08692265700817278\n",
      "9 0.9152 0.07876798194340817\n",
      "10 0.9161 0.07938227970168125\n",
      "11 0.9189 0.07529682797007944\n",
      "12 0.919 0.08166355504407538\n",
      "13 0.9221 0.06494165123617782\n",
      "14 0.9232 0.060956696709127074\n",
      "15 0.9243 0.06693294873547236\n",
      "16 0.9251 0.07654063350524679\n",
      "17 0.9265 0.07322145499318919\n",
      "18 0.9283 0.07225238765282022\n",
      "19 0.929 0.06423808436900896\n",
      "20 0.9309 0.06412209271163288\n",
      "21 0.9326 0.057904877354609224\n",
      "22 0.9342 0.0569937612054843\n",
      "23 0.9348 0.058714992857742926\n",
      "24 0.9359 0.0499382670773902\n"
     ]
    }
   ],
   "source": [
    "num_iter = 25\n",
    "\n",
    "# Training the model\n",
    "for i in range(num_iter) :\n",
    "    for start, end in zip(range(0, len(trX), 128), range(128, len(trX), 128)) :\n",
    "        cost = train(trX[start:end], trY[start:end])\n",
    "    \n",
    "    # Make the prediction\n",
    "    pred = predict(teX)\n",
    "    # Print the result\n",
    "    print(i, np.mean(np.argmax(teY, axis = 1) == pred), cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the result is slightly greater than the last one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
